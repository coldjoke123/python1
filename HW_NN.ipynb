{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldjoke123/python1/blob/master/HW_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UlyZZHMih0S8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNrzjjRtiVqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 載入資料並且重新調整資料的樣子"
      ]
    },
    {
      "metadata": {
        "id": "Emmkq2AZiFv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_test = x_test.reshape(10000,784)\n",
        "y_train = np_utils.to_categorical(y_train,10)\n",
        "y_test = np_utils.to_categorical(y_test,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wiU5BZMsvIGo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 將老師上課的模型匯入"
      ]
    },
    {
      "metadata": {
        "id": "ILKIp990iGCW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "89cjYcOmvPqu",
        "colab_type": "code",
        "outputId": "f38871d7-db40-4680-9d64-580b076aff05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "model_old = Sequential()\n",
        "model_old.add(Dense(4, input_dim = 784))\n",
        "model_old.add(Activation('sigmoid'))\n",
        "model_old.add(Dense(2))\n",
        "model_old.add(Activation('sigmoid'))\n",
        "model_old.add(Dense(10))\n",
        "model_old.add(Activation('softmax'))\n",
        "model_old.compile(loss = 'mse', optimizer=SGD(lr = 0.087), metrics=['accuracy'])\n",
        "model_old.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3397 (Dense)           (None, 4)                 3140      \n",
            "_________________________________________________________________\n",
            "activation_3397 (Activation) (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_3398 (Dense)           (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "activation_3398 (Activation) (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_3399 (Dense)           (None, 10)                30        \n",
            "_________________________________________________________________\n",
            "activation_3399 (Activation) (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,180\n",
            "Trainable params: 3,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nM20M7rsvoza",
        "colab_type": "code",
        "outputId": "26dc1a3d-c2f3-43e7-8e0f-95d9efefe368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "model_old_20 = model_old.fit(x_train, y_train, \n",
        "                         batch_size=100, \n",
        "                         epochs=20,\n",
        "                         verbose=1,\n",
        "                         validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 56s 927us/step - loss: 0.0907 - acc: 0.0991 - val_loss: 0.0903 - val_acc: 0.1009\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0903 - acc: 0.0906 - val_loss: 0.0901 - val_acc: 0.0368\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 16s 265us/step - loss: 0.0901 - acc: 0.0431 - val_loss: 0.0900 - val_acc: 0.0391\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0899 - acc: 0.0523 - val_loss: 0.0898 - val_acc: 0.0674\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0898 - acc: 0.0782 - val_loss: 0.0897 - val_acc: 0.0940\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.0897 - acc: 0.0928 - val_loss: 0.0896 - val_acc: 0.0934\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0896 - acc: 0.0958 - val_loss: 0.0895 - val_acc: 0.0956\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 16s 270us/step - loss: 0.0895 - acc: 0.1333 - val_loss: 0.0894 - val_acc: 0.2001\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.0894 - acc: 0.1959 - val_loss: 0.0893 - val_acc: 0.2023\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.0893 - acc: 0.1967 - val_loss: 0.0893 - val_acc: 0.2041\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0892 - acc: 0.1981 - val_loss: 0.0892 - val_acc: 0.2053\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0892 - acc: 0.1990 - val_loss: 0.0891 - val_acc: 0.2066\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0891 - acc: 0.1996 - val_loss: 0.0890 - val_acc: 0.2054\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0890 - acc: 0.1992 - val_loss: 0.0889 - val_acc: 0.2052\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0889 - acc: 0.2002 - val_loss: 0.0889 - val_acc: 0.2066\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0888 - acc: 0.2008 - val_loss: 0.0888 - val_acc: 0.2062\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0888 - acc: 0.2008 - val_loss: 0.0887 - val_acc: 0.2067\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0886 - acc: 0.2010 - val_loss: 0.0885 - val_acc: 0.2069\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0885 - acc: 0.2014 - val_loss: 0.0883 - val_acc: 0.2065\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0883 - acc: 0.2008 - val_loss: 0.0882 - val_acc: 0.2050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gZ-ZBD9sio0g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 對資料做正規化，將資料調整至 $[0,1]$ 區間\n",
        "\n",
        "降低資料受到極值的影響"
      ]
    },
    {
      "metadata": {
        "id": "dL3itLxOirs1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_reg = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
        "x_test_reg = (x_test - x_test.min())/(x_test.max() - x_test.min())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W-dhtrzvjVOj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 首先，我們增加神經元的個數至每層100個，其餘參數不變"
      ]
    },
    {
      "metadata": {
        "id": "iNDYk7bXiGFX",
        "colab_type": "code",
        "outputId": "46e8224c-3749-411a-eaa0-2b7d2284e431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "model_test1 = Sequential()\n",
        "model_test1.add(Dense(100, input_dim = 784))\n",
        "model_test1.add(Activation('sigmoid'))\n",
        "model_test1.add(Dense(100))\n",
        "model_test1.add(Activation('sigmoid'))\n",
        "model_test1.add(Dense(10))\n",
        "model_test1.add(Activation('softmax'))\n",
        "model_test1.compile(loss = 'mse', optimizer=SGD(lr = 0.087), metrics=['accuracy'])\n",
        "model_test1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3400 (Dense)           (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "activation_3400 (Activation) (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3401 (Dense)           (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "activation_3401 (Activation) (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3402 (Dense)           (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "activation_3402 (Activation) (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,610\n",
            "Trainable params: 89,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sFOf_V_wiGH_",
        "colab_type": "code",
        "outputId": "4e54216b-8d49-4e87-bc37-732b78959909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "model_test1_20 = model_test1.fit(x_train_reg, y_train, \n",
        "                         batch_size=100, \n",
        "                         epochs=20,\n",
        "                         verbose=1,\n",
        "                         validation_data=(x_test_reg, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 56s 941us/step - loss: 0.0911 - acc: 0.1325 - val_loss: 0.0899 - val_acc: 0.1183\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0897 - acc: 0.1429 - val_loss: 0.0895 - val_acc: 0.1897\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0894 - acc: 0.1962 - val_loss: 0.0893 - val_acc: 0.1762\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0892 - acc: 0.2014 - val_loss: 0.0891 - val_acc: 0.2722\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0890 - acc: 0.2467 - val_loss: 0.0889 - val_acc: 0.2794\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 17s 287us/step - loss: 0.0888 - acc: 0.2668 - val_loss: 0.0887 - val_acc: 0.3052\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0886 - acc: 0.2904 - val_loss: 0.0884 - val_acc: 0.2951\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0883 - acc: 0.3002 - val_loss: 0.0881 - val_acc: 0.3059\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0880 - acc: 0.3095 - val_loss: 0.0878 - val_acc: 0.3088\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0877 - acc: 0.3111 - val_loss: 0.0875 - val_acc: 0.3331\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0873 - acc: 0.3227 - val_loss: 0.0870 - val_acc: 0.3284\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0869 - acc: 0.3182 - val_loss: 0.0866 - val_acc: 0.3478\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0863 - acc: 0.3287 - val_loss: 0.0859 - val_acc: 0.3191\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 17s 290us/step - loss: 0.0857 - acc: 0.3220 - val_loss: 0.0852 - val_acc: 0.3191\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.0849 - acc: 0.3198 - val_loss: 0.0843 - val_acc: 0.3275\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0839 - acc: 0.3236 - val_loss: 0.0832 - val_acc: 0.3159\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0827 - acc: 0.3219 - val_loss: 0.0818 - val_acc: 0.3311\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0812 - acc: 0.3309 - val_loss: 0.0802 - val_acc: 0.3513\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0794 - acc: 0.3520 - val_loss: 0.0783 - val_acc: 0.3677\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0775 - acc: 0.3789 - val_loss: 0.0762 - val_acc: 0.3917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ld8CNENM0RD_",
        "colab_type": "code",
        "outputId": "93a9aba0-9dea-455a-f762-744d76962252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "score = model_test1.evaluate(x_test_reg, y_test)\n",
        "print('loss:', score[0])\n",
        "print('accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 681us/step\n",
            "loss: 0.07623723120689392\n",
            "accuracy: 0.3917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rrNPgtVMzym_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 相較於老師上課的模型，可以發現增加神經元數是能增加準確率的！\n",
        "\n",
        "## 於是，在Hidden layer有兩層activation function是sigmoid，loss function是mse，optimizer是SGD的條件下，測試神經元的個數以50個為單位從100到800來尋找準確率最高的組合。"
      ]
    },
    {
      "metadata": {
        "id": "eGgb4J8Y0C35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seek_opt(n):\n",
        "    seek_accu = np.zeros((2*(n-1),n-1))\n",
        "    for i in range(1,n):\n",
        "        for j in range(1,n):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(50*(i+1), input_dim = 784))\n",
        "            model.add(Activation('sigmoid'))\n",
        "            model.add(Dense(50*(j+1)))\n",
        "            model.add(Activation('sigmoid'))\n",
        "            model.add(Dense(10))\n",
        "            model.add(Activation('softmax'))\n",
        "            model.compile(loss = 'mse', optimizer=SGD(lr = 0.087), metrics=['accuracy'])\n",
        "            score = model.evaluate(x_test_reg, y_test)\n",
        "            seek_accu[i-1,j-1] = score[1]\n",
        "            seek_accu[i-1+(n-1),j-1] = score[0]\n",
        "    return seek_accu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Ka8SYth0C9P",
        "colab_type": "code",
        "outputId": "f5238ea8-8d2f-474e-8a62-2b801d13a9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3842
        }
      },
      "cell_type": "code",
      "source": [
        "seek_accu = seek_opt(16)\n",
        "accu = seek_accu[:16,:]\n",
        "loss = seek_accu[16:,:]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 43s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 44s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 45s 4ms/step\n",
            "10000/10000 [==============================] - 45s 5ms/step\n",
            "10000/10000 [==============================] - 45s 5ms/step\n",
            "10000/10000 [==============================] - 45s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 45s 5ms/step\n",
            "10000/10000 [==============================] - 45s 5ms/step\n",
            "10000/10000 [==============================] - 45s 5ms/step\n",
            "10000/10000 [==============================] - 45s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 42s 4ms/step\n",
            "10000/10000 [==============================] - 42s 4ms/step\n",
            "10000/10000 [==============================] - 42s 4ms/step\n",
            "10000/10000 [==============================] - 42s 4ms/step\n",
            "10000/10000 [==============================] - 42s 4ms/step\n",
            "10000/10000 [==============================] - 41s 4ms/step\n",
            "10000/10000 [==============================] - 42s 4ms/step\n",
            "10000/10000 [==============================] - 42s 4ms/step\n",
            "10000/10000 [==============================] - 46s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 47s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 48s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 51s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 49s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 51s 5ms/step\n",
            "10000/10000 [==============================] - 51s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 51s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 51s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 50s 5ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 51s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 52s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 53s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 6ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 54s 5ms/step\n",
            "10000/10000 [==============================] - 55s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 55s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 5ms/step\n",
            "10000/10000 [==============================] - 55s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 55s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n",
            "10000/10000 [==============================] - 56s 6ms/step\n",
            "10000/10000 [==============================] - 57s 6ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pW8VJKoY0DUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt_acc_value = accu.max()\n",
        "opt_acc_index = accu.argmax()\n",
        "\n",
        "model_best = Sequential()\n",
        "model_best.add(Dense(50*((opt_acc_index//15)+2), input_dim = 784))\n",
        "model_best.add(Activation('sigmoid'))\n",
        "model_best.add(Dense(50*((opt_acc_index%15)+2))\n",
        "model_best.add(Activation('sigmoid'))\n",
        "model_best.add(Dense(10))\n",
        "model_best.add(Activation('softmax'))\n",
        "model_best.compile(loss = 'mse', optimizer=SGD(lr = 0.087), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VXamWxHv6yx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_best_20 = model_best.fit(x_train_reg, y_train,\n",
        "                         batch_size = 100,\n",
        "                         epochs = 20,\n",
        "                         verbose = 1,\n",
        "                         validation_data = (x_test_reg, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Za9Qce87pzYA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 由以上結果可知，在兩層hidden layer下，神經元個數分別為 跟 會有最高的準確率！\n",
        "\n",
        "## 比較新的模型與老師上課的模型"
      ]
    },
    {
      "metadata": {
        "id": "hKYfYQIC0DXF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(model_old_20.history[\"acc\"])\n",
        "plt.plot(model_best_20.history[\"acc\"])\n",
        "         \n",
        "plt.title(\"model training accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"model_old\",\"model_best\"], loc = 'best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dk96VUlOr-zX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(model_old_20.history[\"val_loss\"])\n",
        "plt.plot(model_best_20.history[\"val_loss\"])\n",
        "         \n",
        "plt.title(\"model validation loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"model_old\",\"model_best\"], loc = 'best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Osi8TE-0wyrK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 在這個參數的基礎下去選擇不同的loss function 與activation function做測試"
      ]
    },
    {
      "metadata": {
        "id": "FQUjiUWDwi5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}